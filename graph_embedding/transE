<file><code>from pycuke1 import transform, run
from pycuke1.codegen import *
from pycuke1.helpers import ASGTraversal, IRTraversal, flatten, get_obj
from pycuke1.transform.fuse import basic_rule, fuse_operators
from pycuke1.asg import *
from pycuke1.asg2ir import gen_ir
from pycuke1.ir import *</code><text>Here we define the score function of the TransE model. The function reads the entity embeddings from the heads and tails of a batch of edges (&lt;tt&gt;vh&lt;/tt&gt; and &lt;tt&gt;vt&lt;/tt&gt;), and the relation embeddings of the edges (&lt;tt&gt;vr&lt;/tt&gt;). The score function is *vh - vt + vr*.</text><code>def transE():
    nnodes = Var(name='nnodes')
    nedges = Var(name='nedges')
    dim = Var(name='dim')
    batch_size = Var(name='batch_size')
    Eemb = Tensor((nnodes, dim), name='Eemb')
    Remb = Tensor((nedges, dim), name='Remb')
    h = Tensor((batch_size, ), dtype='int', name='h')
    t = Tensor((batch_size, ), dtype='int', name='t')
    r = Tensor((batch_size, ), dtype='int', name='r')
    r.attr['reuse'] = True
    vh = Eemb[h]
    vt = Eemb[t]
    vr = Remb[r]
    
    # TransE: Eemb[h] - Eemb[t] + Remb[r]
    res = vh - vt + vr
    code = gpu.print_cuda(gen_ir(res))
    print(code)</code><code>transform.passes = []
transE()</code><text>In the above code, two loops are generated to compute *vh - vt* and the *(vh - vt) + vr* separately. We can fuse the two loops by defining a fuser with our fusion rules.</text><code>def fuse_rule(node, res):
    '''
    Our specific loop fusion strategies for KGE.
    We can fuse some specific operators to reduce memory access and other overhead.

    Parameters:
    node (asg.ASTNode): root node of user-defined operation.
    '''
    if type(node) == TensorOp and 'op_name' in node.attr and node.attr['op_name'] == 'bvv':
        # fuse bvv and sub-operatos (elementwise or bvm)
        if type(node.operators[1]) == TensorOp and len(node.operators[1].ref_by) == 1:
            if node.operators[1].op_type in elementwise_op or ('op_name' in node.operators[1].attr and node.operators[1].attr['op_name'] == 'bvm'):
                fuse_operators(node, node.input_orders[1], node.operators[1])

        if type(node.operators[2]) == TensorOp and len(node.operators[2].ref_by) == 1:
            if node.operators[2].op_type in elementwise_op or ('op_name' in node.operators[2].attr and node.operators[2].attr['op_name'] == 'bvm'):
                fuse_operators(node, node.input_orders[2], node.operators[2])

    if type(node) == TensorOp and 'op_name' in node.attr and node.attr['op_name'] in ['bsv']:
        # fuse bsv and sub-operators (elementwise or bvv or sum) and (elementwise or bvm)
        if type(node.operators[1]) == TensorOp and len(node.operators[1].ref_by) == 1:
            if node.operators[1].op_type in elementwise_op or ('op_name' in node.operators[1].attr and node.operators[1].attr['op_name'] in ['bvv', 'sum']):
                fuse_operators(node, node.input_orders[1], node.operators[1])

        if type(node.operators[2]) == TensorOp and len(node.operators[2].ref_by) == 1:
            if node.operators[2].op_type in elementwise_op or ('op_name' in node.operators[2].attr and node.operators[2].attr['op_name'] == 'bvm'):
                fuse_operators(node, node.input_orders[2], node.operators[2])
        
    if type(node) == TensorOp and 'op_name' in node.attr and node.attr['op_name'] == 'bsm':
        # fuse bsm and sub-operators (elementwise or bvv) and (elementwise or bov)
        if type(node.operators[1]) == TensorOp and len(node.operators[1].ref_by) == 1:
            if node.operators[1].op_type in elementwise_op or ('op_name' in node.operators[1].attr and node.operators[1].attr['op_name'] == 'bvv'):
                fuse_operators(node, node.input_orders[1], node.operators[1])

        if type(node.operators[2]) == TensorOp and len(node.operators[2].ref_by) == 1:
            if node.operators[2].op_type in elementwise_op or ('op_name' in node.operators[2].attr and node.operators[2].attr['op_name'] == 'bov'):
                fuse_operators(node, node.input_orders[2], node.operators[2])

    if type(node) == TensorOp and 'op_name' in node.attr and node.attr['op_name'] == 'bov':
        # fuse bov and sub-operators (elementwise)
        if type(node.operators[1]) == TensorOp and len(node.operators[1].ref_by) == 1:
            if node.operators[1].op_type in elementwise_op:
                fuse_operators(node, node.input_orders[1], node.operators[1])

        if type(node.operators[2]) == TensorOp and len(node.operators[2].ref_by) == 1:
            if node.operators[2].op_type in elementwise_op:
                fuse_operators(node, node.input_orders[2], node.operators[2])

class fuser:
    '''
    Transformation pass for loop fusion for the given asgnode.
    basic_rule is default fusion rule in cuKE, fuse_rule is our specific loop fusion strategies.
    '''
    def __init__(self):
        self.rules = [basic_rule, fuse_rule]

    def __call__(self, node):
        def action(n, res):
            for r in self.rules:
                r(n, res)
        t = ASGTraversal(action)
        t(node)
        return node</code><code>transform.passes = [fuser()]
transE()</code><text>The code is fused within one loop but is inefficient for parallel programming. To improve the efficiency of the generated code, we first need to tile the loops.</text><code>class tiler():
    '''
    Transformation pass for loop tiling and parallelization for the given asgnode.
    We tile 3-level loops and apply different parallel hierarchy to each loop for parallelization.
    '''
    def __init__(self, C, D):
        self.C = C
        self.D = D
        self.flag = []

    def __call__(self, node):
        def action(n, res):
            if isinstance(n, TensorOp) and 'op_name' in n.attr:
                self.flag.append(n.attr['op_name'])
            if not 'scope' in n.attr and len(n.compute) &gt; 0:
                transform.split.split_level(n, self.C, 0)
                transform.parallelize.parallelize_loop(n, 80, [0])
                transform.parallelize.parallelize_loop(n, 16, [0, 0])
                transform.split.split_level(n, self.D, 2)
                transform.parallelize.parallelize_level(n, 64, 3)
                if 'bvm' in self.flag:
                    transform.split.split_level(n, self.D, 4)
                    transform.interchange.interchange(n, [3, 4])

        t = ASGTraversal(action)
        t(node)
        return node</code><code>transform.passes = [fuser(), tiler(16, 64)]
transE()</code><text>All the data are stored in GPU global memory, to achieve better performance, we can transfer some of the intermediate data to the GPU shared memory.</text><code>class smem():
    '''
    Transformation pass for adding memory optimization for the given asgnode.
    We will move intermediate results into GPU shared memory to reduce global memory traffic.
    '''
    def __init__(self, C, D):
        self.C = C
        self.D = D

    def __call__(self, node):
        def action(n, res):
            if type(n) == TensorOp and 'op_name' in n.attr and n.attr['op_name'] in ['bsv', 'bvv', 'bvm', 'sum']:
                
                if n.compute:
                    transform.cuda_smem.add_direct_cache(node, n.eval)
                else:
                    def action(nn, res):
                        transform.cuda_smem.add_direct_cache(nn, n.eval)
                        if nn.ref_by:
                            for i in nn.ref_by:
                                    transform.cuda_smem.add_direct_cache(i, n.eval)
                    ASGTraversal(action)(node)
                
            def get_assigns(s, res):
                if type(s) in (Assignment, Code):
                    res.append(s)
                return [True, True, True, True, True]
                
            def _find_reduction_loop(loop):
                def action(s, res):
                    if isinstance(s, Loop):
                        if 'ptype' in s.attr and s.attr['ptype'] == 'reduction':
                            res.append(s)
                    return [True, True, True, True, True]

                r = IRTraversal(action)(loop)
                return r

            if type(n) == TensorOp and 'op_name' in n.attr and n.attr['op_name'] in ['bvv', 'sum']:
                def recurrent_call(nn):
                    scope = flatten(nn.compute)
                    for loop in scope:
                        redu_loop = _find_reduction_loop(loop)
                        for s in redu_loop:
                            assign = IRTraversal(get_assigns)(s)
                            # get lhs of last assignment
                            lhs = assign[-1].lhs
                            obj = get_obj(lhs)
                            if obj in n.eval.attr['storage']:
                                transform.cuda_smem.add_direct_cache(nn, obj)
                    for i in nn.ref_by:
                        recurrent_call(i)
                recurrent_call(n)
            
        self.eval = node.eval
        t = ASGTraversal(action)(node)
        return node</code><code>transform.passes = [fuser(), tiler(16, 64), smem(16, 64)]
transE()</code><text>We can improve data access efficiency by our runtime inspection, which helps us to reduce the overhead of loading redundant embeddings.</text><code>class indirect_smem():
    '''
    Transformation pass for adding memory optimization for the given asgnode.
    We will load indirect memory access data into GPU shared memory to reuse indices and reduce global memory traffic.
    '''
    def __init__(self, C, D):
        self.C = C
        self.D = D

    def __call__(self, node):
        def action(n, res):
            if type(n) == TensorOp and n.op_type == 'index' and 'reuse' in n.operators[1].attr and n.operators[1].attr['reuse'] == True:
                unique_idx = Tensor((n.operators[1]._size()[0]/self.C, self.C), dtype='int', name=n.operators[1].name+'_uniq')
                buf_idx = Tensor((n.operators[1]._size()[0]/self.C, self.C), dtype='int', name=n.operators[1].name+'_buf')
                unique_cnt = Tensor((n.operators[1]._size()[0]/self.C, ), dtype='int', name=n.operators[1].name+'_unique_cnt')
                n.operators[1].attr['idx'] = [[unique_idx.name, unique_idx], [buf_idx.name, buf_idx], [unique_cnt.name, unique_cnt]]
                
                transform.cuda_smem.add_indirect_cache(node, n, self.C, self.D, unique_idx, buf_idx, unique_cnt)
                def action(nn, res):
                    transform.cuda_smem.add_indirect_cache(nn, n, self.C, self.D, unique_idx, buf_idx, unique_cnt)
                    if nn.ref_by:
                        for i in nn.ref_by:
                            transform.cuda_smem.add_indirect_cache(i, n, self.C, self.D, unique_idx, buf_idx, unique_cnt)
                ASGTraversal(action)(node)
                
        self.eval = node.eval
        t = ASGTraversal(action)(node)
        return node</code><code>transform.passes = [fuser(), tiler(16, 64), smem(16, 64), indirect_smem(16, 64)]
transE()</code></file>